{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of _4_qlearning_agent.ipynb","provenance":[{"file_id":"1_BzVj47VU3SeEKCNb4jqh39uTyrIEKqY","timestamp":1592400937660}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"MPD-_GwinU9D","colab_type":"code","colab":{}},"source":["#   ____                            _____     _____                   __          __        _        _                 \n","#  / __ \\                     /\\   |_   _|   / ____|                  \\ \\        / /       | |      | |                \n","# | |  | |_ __   ___ _ __    /  \\    | |    | |  __ _   _ _ __ ___     \\ \\  /\\  / /__  _ __| | _____| |__   ___  _ __  \n","# | |  | | '_ \\ / _ \\ '_ \\  / /\\ \\   | |    | | |_ | | | | '_ ` _ \\     \\ \\/  \\/ / _ \\| '__| |/ / __| '_ \\ / _ \\| '_ \\ \n","# | |__| | |_) |  __/ | | |/ ____ \\ _| |_   | |__| | |_| | | | | | |     \\  /\\  / (_) | |  |   <\\__ \\ | | | (_) | |_) |\n","#  \\____/| .__/ \\___|_| |_/_/    \\_\\_____|   \\_____|\\__, |_| |_| |_|      \\/  \\/ \\___/|_|  |_|\\_\\___/_| |_|\\___/| .__/ \n","#        | |                                         __/ |                                                      | |    \n","#        |_|                                        |___/                                                       |_|    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jpSPqQInnbnr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"ok","timestamp":1592233267254,"user_tz":240,"elapsed":4036,"user":{"displayName":"Simon Landry-Pellerin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h6-r8DPWprYL87H9ZK1Eq8BiMpDSZIZ7Iyrb8Q=s64","userId":"16311999124761086587"}},"outputId":"c121ce5a-79a1-4d63-d999-8298f3ce415d"},"source":["!pip install gym\n","#!pip install torch==1.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jks-29yFneXd","colab_type":"code","colab":{}},"source":["import json\n","from itertools import count\n","\n","import gym\n","from gym import spaces\n","\n","import torch\n","from torch import Tensor\n","\n","import random\n","from random import randint\n","\n","from typing import Dict, List, Optional, Callable, Any, Tuple, Union, Type\n","\n","import pathlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoT13geAg5mK","colab_type":"code","colab":{}},"source":["class RxQLearningAgent:\n","  def __init__(\n","    self, lr: float, gamma: float, act_space: spaces.Discrete,\n","    obs_space: spaces.Discrete, eps_fn_steps: int\n","  ):\n","    assert obs_space.dtype == int  # compatibility assertion\n","\n","    self.lr = lr  # learning rate\n","    self.gamma = gamma  # gamma parameter\n","    self.qtable: Dict[int, Tensor] = dict()  # Q-table with time-step as keys for a 1D Q-table\n","    self.n_actions = act_space.n  # number of actions\n","\n","    self.eps_fn_steps = eps_fn_steps  # number of steps (episodes) in epsilon log-space\n","    self.eps_fn: Callable[[int], float] = build_epsilon_fn(eps_fn_steps)  # build our epsilon function\n","    self.episode_i: Optional[int] = None  # keep track of the episode number (e.g. for epsilon)\n","    self.eps: Optional[float] = None  # epsilon value as a property of our agent\n","\n","  def act(self, o: int, eps: Optional[float] = None) -> int:  # function to choose action, with optinal epsilon argument\n","    if eps is None:\n","      eps = self.eps  # not epsilon as argument, use agent's current epsilon value\n","\n","    if o not in self.qtable:\n","      self.qtable[o] = torch.zeros(self.n_actions)  # add new Q-table for new obs, Q-values are zero\n","\n","    if random.uniform(1.0, 0.0) < eps:  # epsilon-greedy condition for exploration\n","      return self.explore(o)  # exploring random action\n","\n","    return self.qtable[o].argmax().item()  # return frequency with highest Q-value\n","\n","  def explore(self, o: int) -> int:  # action-space exploration\n","    return randint(0, self.n_actions - 1)  # random action\n","\n","  def update(self, o: int, a: int, r: float, o_prime: int) -> float:  # agent update function (e.g. Q-learning update)\n","    if o not in self.qtable:\n","      self.qtable[o] = torch.zeros(self.n_actions)  # add new Q-table for new obs, Q-values are zero\n","\n","    if o_prime not in self.qtable:\n","      self.qtable[o_prime] = torch.zeros(self.n_actions)  # add new Q-table for new obs, Q-values are zero\n","\n","    old_o_a_value = self.qtable[o][a].item()  # keep old Q-values to compute delta update\n","\n","    td_target = r + self.gamma * self.qtable[o_prime].max().item()  # compute Temporal-Difference target (e.i [r + gamma * Q(o', a_max)])\n","    self.qtable[o][a] += self.lr * (td_target - self.qtable[o][a].item())  # update Q-table (e.i. Q(o, s) <- Q(o, s) + lr * [TD-target - Q(o, a)])\n","\n","    return self.qtable[o][a].item() - old_o_a_value  # return delta update to training loop\n","\n","  def episode_reset(self, o: int, episode_i: int) -> int:  # reset agent for a new episode\n","    self.episode_i = episode_i  # current episode number\n","    self.eps = self.eps_fn(episode_i)  # epsilon w.r.t. episode number\n","    return self.act(o, self.eps)  # act upon initial observation\n","\n","  def save_to_file(self, path: str, overwrite: bool = False):  # save agent state to file\n","    file = pathlib.Path(path)  # file path\n","    if not overwrite and file.exists():  # exception if file exist and overwrite is false\n","      raise FileExistsError()\n","\n","    # agent state as a python dictionary\n","    agent_state: Dict[str, Any] = {\n","      'gamma': self.gamma, 'n_actions': self.n_actions,\n","      'qtable': {k: v.tolist() for k, v in self.qtable.items()}\n","    }\n","\n","    with file.open('w') as f:  # open file to write in it\n","      json.dump(agent_state, f)  # write agent state in file\n","\n","  def load_from_file(self, path: str):  # load agent state from compatible saved state\n","    file = pathlib.Path(path)  # file path\n","    if not file.exists():\n","      return  # file does not exists\n","\n","    with file.open('r') as f:  # open file to read it\n","      agent_state = json.load(f)  # load agent state from file\n","\n","    self.gamma = agent_state['gamma']  # set gamma to loaded value\n","    self.n_actions = agent_state['n_actions']  # set number of actions from loaded value\n","    # set Q-table from loaded values\n","    self.qtable = {\n","      int(k): torch.tensor(v) for k, v in agent_state['qtable'].items()\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJYTWtphoSFY","colab_type":"code","colab":{}},"source":["class TxWirelessGym(gym.Env):\n","  def __init__(self, n_freqs: int, horizon: int):\n","    super(TxWirelessGym, self).__init__()  # initialize gym.Env base class\n","    self.action_space: spaces.Discrete = spaces.Discrete(n_freqs)  # action space {0, 1, ..., n_freqs - 1}\n","    self.observation_space: spaces.Discrete = spaces.Discrete(horizon + 1)  # observation space {0, 1, ..., horizon}\n","    self.hor = horizon  # gym horizon to know when we are DONE\n","    self.tx_freq = torch.randint(n_freqs, size=(horizon,))  # random Tx frequencies w.r.t. time-step\n","    self.t = 0  # initial time-step / observation \n","\n","  def step(self, action: int):  # step function to interact with the gym\n","    if self.t < self.hor:  # non-terminal observation, horizon not reached\n","      r = float(action == self.tx_freq[self.t])  # float(True) = 1.0 if Tx and Rx frequencies are the same\n","    else:  # gym horizon reached\n","      r = 0.0\n","    self.t += 1  # increment our time-step / observation\n","    o = self.t  # observation that will return\n","    done = (self.t == self.hor)  # is terminal gym state reached\n","    return o, r, done, {}  # gyms always returns <obs, reward, terminal obs reached, debug/info dictionary>\n","\n","  def reset(self):  # reset our gym for a new episode\n","    self.t = 0  # initial time-step\n","    o = self.t  # initial observation\n","    return o\n","\n","  def render(self, mode='human'):  # gym visual rendering (e.g. text, image, plot, 3D frame, etc.)\n","    a_r = []\n","    for freq in range(self.action_space.n):  # try every freqs\n","      if self.t < self.hor:\n","        a_r.append(float(freq == self.tx_freq[self.t]))  # float(True) = 1.0 if Tx and Rx frequencies are the same\n","      else:\n","        a_r.append(0.0)\n","    # print all action-reward for this time-step\n","    print(\n","      't={o:2}, tx_freq:{tx_freq}, tx_rewards:{action_rewards}'.format(\n","        o=self.t, tx_freq=[i for i in range(len(a_r)) if a_r[i] == 1.0],\n","        action_rewards=a_r\n","      )\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyZZrx_GtXPi","colab_type":"code","colab":{}},"source":["def build_epsilon_fn(steps: int) -> Callable[[int], float]:  # epsilon function builder\n","  log_epsilon_space = torch.logspace(1.0, -1.0, steps=steps, base=10.0) / 10.0  # epsilon log-space {1.0, ..., 0.0}\n","\n","  def epsilon_fn(episode_i: int):  # epsilon function\n","    if episode_i >= steps:  # max step reached\n","      return 0.0\n","    return log_epsilon_space[episode_i].item()  # return epsilon value\n","\n","  return epsilon_fn  # return our function as an callable object"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XizJyfrSoN8g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592233270238,"user_tz":240,"elapsed":6962,"user":{"displayName":"Simon Landry-Pellerin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2h6-r8DPWprYL87H9ZK1Eq8BiMpDSZIZ7Iyrb8Q=s64","userId":"16311999124761086587"}},"outputId":"3b03b0f9-6b45-45cd-8ee2-852f1aa298f4"},"source":["random.seed(1234)  # python random number generator seed\n","torch.manual_seed(1234)  # pytorch random number generator seed\n","\n","n_freqs = 10  # number of frequencies\n","horizon = 15  # horizon of our gym (episodes)\n","\n","gym_env = TxWirelessGym(n_freqs, horizon)  # our gym environment\n","\n","lr = 0.15  # learning rate\n","gamma = 0.0  # gamma parameter\n","eps_fn_steps = 300  # number of steps (episodes) in epsilon log-space\n","# initialize our Rx agent with learning parameters and gym parameters\n","agent = RxQLearningAgent(\n","  lr, gamma, gym_env.action_space, gym_env.observation_space, eps_fn_steps\n",")\n","\n","agent.load_from_file('rx_agent_1.json')  # load previously saved agent\n","\n","show_verbose = False  # print information for debugging\n","render_gym = False  # render gym\n","\n","# print simulation parameters\n","print(\n","  'model: {{n_freqs={n_freqs}, horizon={horizon}}}, '\n","  'agent: {{lr={lr}, gamma={gamma}, eps_fn_steps={eps_fn_steps}}}'.format(\n","    n_freqs=gym_env.action_space.n, horizon=horizon, lr=lr, gamma=gamma,\n","    eps_fn_steps=eps_fn_steps\n","  )\n",")\n","\n","running_len = 5  # length of our running training data\n","running_delta = []  # running delta (e.g. the last running_len delta update)\n","running_acc = []  # running accuracy (e.g. the last running_len accuracy)\n","\n","for episode_i in count():  # training loop\n","  if show_verbose:\n","    print('starting episode {episode_i}...'.format(episode_i=episode_i))\n","\n","  delta_update: List[float] = []  # delta update of our Q-table\n","  n_successes: int = 0  # number of optimal actions (actions with maximum reward)\n","  cumul_r: float = 0.0  # cumulative reward\n","\n","  o = gym_env.reset()  # reset gym for new episode\n","  a = agent.episode_reset(o, episode_i)  # reset agent with initial gym observation and episode number\n","\n","  for t in count():  # episode loop\n","    if render_gym:\n","      gym_env.render()  # show gym rendering\n","    o_prime, r, done, _ = gym_env.step(a)  # interact with the gym, get environment transition\n","\n","    delta_update.append(agent.update(o, a, r, o_prime))  # update agent with transition, get delta update\n","    cumul_r += r  # add reward to cumulative reward\n","    n_successes += int(r > 0.0)  # success if optimal action-reward of 1.0\n","\n","    if show_verbose:   # show transition of our model (e.i. <o, a, r, o'>)\n","      print(\n","        'transition=<{o}, {a}, {r}, {o_prime}>,'\n","        ' delta_update={delta}'.format(\n","          o=o, a=a, r=r, o_prime=o_prime, delta=delta_update[-1]\n","        )\n","      )\n","\n","    o = o_prime  # increment to next observation (e.g. next observation)\n","    a = agent.act(o)  #  act upon next observation\n","    if done:\n","      break  # terminal gym observation reached, out of horizon\n","\n","  # strip running data because we reached running length\n","  if len(running_acc) >= running_len or len(running_delta) >= running_len:\n","    running_acc.pop(0)  # delete oldest running accuracy\n","    running_delta.pop(0)  # delete oldest running update delta\n","\n","  running_acc.append(n_successes / horizon)  # add latest accuracy to running data\n","  running_delta.append(sum(delta_update))  # add latest update delta to running data\n","\n","  # show episode results\n","  print(\n","    'episode {episode_i}: cumul_reward={cumul_r}, accuracy:{acc:0.5}, '\n","    'cumul_delta={cumul_delta:0.5}, eps={eps:0.5}'.format(\n","      episode_i=episode_i, cumul_r=cumul_r, acc=running_acc[-1],\n","      cumul_delta=running_delta[-1], eps=agent.eps\n","    )\n","  )\n","\n","  # training stop conditions\n","  if (\n","    all([acc == 1.0 for acc in running_acc])  # all running accuracy are maximized\n","    and all([delta < 0.0001 for delta in running_delta])  # all running delta update a lower than 0.0001\n","    and episode_i >= running_len  # running data have reached running length\n","  ) or episode_i >= eps_fn_steps + running_len:  # epsilon was 0 for all running data (nothing will change)\n","    break  # exit training loop\n","\n","agent.save_to_file('rx_agent_1.json', overwrite=True)  # save agent state to file\n","gym_env.close()  # close gym environment"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model: {n_freqs=10, horizon=15}, agent: {lr=0.15, gamma=0.0, eps_fn_steps=300}\n","episode 0: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.15, eps=1.0\n","episode 1: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.15, eps=0.98472\n","episode 2: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.15, eps=0.96967\n","episode 3: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.15, eps=0.95485\n","episode 4: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.4275, eps=0.94025\n","episode 5: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.1275, eps=0.92588\n","episode 6: cumul_reward=2.0, accuracy:0.13333, cumul_delta=0.25837, eps=0.91173\n","episode 7: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.15, eps=0.8978\n","episode 8: cumul_reward=4.0, accuracy:0.26667, cumul_delta=0.54212, eps=0.88407\n","episode 9: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.405, eps=0.87056\n","episode 10: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.3558, eps=0.85726\n","episode 11: cumul_reward=2.0, accuracy:0.13333, cumul_delta=0.23587, eps=0.84415\n","episode 12: cumul_reward=1.0, accuracy:0.066667, cumul_delta=0.1275, eps=0.83125\n","episode 13: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.36337, eps=0.81855\n","episode 14: cumul_reward=2.0, accuracy:0.13333, cumul_delta=0.15867, eps=0.80604\n","episode 15: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.60132, eps=0.79372\n","episode 16: cumul_reward=2.0, accuracy:0.13333, cumul_delta=0.21962, eps=0.78159\n","episode 17: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.29792, eps=0.76964\n","episode 18: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.48173, eps=0.75788\n","episode 19: cumul_reward=4.0, accuracy:0.26667, cumul_delta=0.30353, eps=0.74629\n","episode 20: cumul_reward=2.0, accuracy:0.13333, cumul_delta=0.16495, eps=0.73489\n","episode 21: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.49716, eps=0.72366\n","episode 22: cumul_reward=4.0, accuracy:0.26667, cumul_delta=0.35466, eps=0.7126\n","episode 23: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.52245, eps=0.7017\n","episode 24: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.38935, eps=0.69098\n","episode 25: cumul_reward=4.0, accuracy:0.26667, cumul_delta=0.3297, eps=0.68042\n","episode 26: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.32768, eps=0.67002\n","episode 27: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.47819, eps=0.65978\n","episode 28: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.35397, eps=0.64969\n","episode 29: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.54144, eps=0.63976\n","episode 30: cumul_reward=3.0, accuracy:0.2, cumul_delta=0.17121, eps=0.62999\n","episode 31: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.24162, eps=0.62036\n","episode 32: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.2913, eps=0.61088\n","episode 33: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.30096, eps=0.60154\n","episode 34: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.18984, eps=0.59235\n","episode 35: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.21188, eps=0.58329\n","episode 36: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.17205, eps=0.57438\n","episode 37: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.12727, eps=0.5656\n","episode 38: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.31936, eps=0.55695\n","episode 39: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.2503, eps=0.54844\n","episode 40: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.38216, eps=0.54006\n","episode 41: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.089456, eps=0.53181\n","episode 42: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.17901, eps=0.52368\n","episode 43: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.12788, eps=0.51567\n","episode 44: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.26038, eps=0.50779\n","episode 45: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.09267, eps=0.50003\n","episode 46: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.10934, eps=0.49239\n","episode 47: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.070145, eps=0.48486\n","episode 48: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.19982, eps=0.47745\n","episode 49: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.21061, eps=0.47015\n","episode 50: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.1206, eps=0.46297\n","episode 51: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.071607, eps=0.45589\n","episode 52: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.091224, eps=0.44893\n","episode 53: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.048555, eps=0.44206\n","episode 54: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.059212, eps=0.43531\n","episode 55: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.060689, eps=0.42865\n","episode 56: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.10846, eps=0.4221\n","episode 57: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.10157, eps=0.41565\n","episode 58: cumul_reward=7.0, accuracy:0.46667, cumul_delta=0.073512, eps=0.4093\n","episode 59: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.023746, eps=0.40304\n","episode 60: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.056834, eps=0.39688\n","episode 61: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.056435, eps=0.39082\n","episode 62: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.052787, eps=0.38484\n","episode 63: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.054345, eps=0.37896\n","episode 64: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.021433, eps=0.37317\n","episode 65: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.041788, eps=0.36747\n","episode 66: cumul_reward=9.0, accuracy:0.6, cumul_delta=0.024695, eps=0.36185\n","episode 67: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.04267, eps=0.35632\n","episode 68: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.018788, eps=0.35087\n","episode 69: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.036578, eps=0.34551\n","episode 70: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.02351, eps=0.34023\n","episode 71: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.015766, eps=0.33503\n","episode 72: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.023192, eps=0.32991\n","episode 73: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.021515, eps=0.32487\n","episode 74: cumul_reward=8.0, accuracy:0.53333, cumul_delta=0.0058581, eps=0.3199\n","episode 75: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.0093104, eps=0.31501\n","episode 76: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.017074, eps=0.3102\n","episode 77: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0079845, eps=0.30546\n","episode 78: cumul_reward=5.0, accuracy:0.33333, cumul_delta=0.0042527, eps=0.30079\n","episode 79: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.0062415, eps=0.29619\n","episode 80: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.012842, eps=0.29166\n","episode 81: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.003365, eps=0.28721\n","episode 82: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.0094208, eps=0.28282\n","episode 83: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0081676, eps=0.27849\n","episode 84: cumul_reward=6.0, accuracy:0.4, cumul_delta=0.0013168, eps=0.27424\n","episode 85: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.0019239, eps=0.27005\n","episode 86: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0073945, eps=0.26592\n","episode 87: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.0015991, eps=0.26185\n","episode 88: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.0053806, eps=0.25785\n","episode 89: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0049793, eps=0.25391\n","episode 90: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.0040842, eps=0.25003\n","episode 91: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0036259, eps=0.24621\n","episode 92: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0011773, eps=0.24245\n","episode 93: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.002956, eps=0.23874\n","episode 94: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.002695, eps=0.23509\n","episode 95: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0022262, eps=0.2315\n","episode 96: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0019772, eps=0.22796\n","episode 97: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0016183, eps=0.22448\n","episode 98: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0013105, eps=0.22105\n","episode 99: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0012308, eps=0.21767\n","episode 100: cumul_reward=11.0, accuracy:0.73333, cumul_delta=0.00048971, eps=0.21434\n","episode 101: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0010835, eps=0.21106\n","episode 102: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00079763, eps=0.20784\n","episode 103: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.00073314, eps=0.20466\n","episode 104: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00057417, eps=0.20153\n","episode 105: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0005734, eps=0.19845\n","episode 106: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.00052863, eps=0.19542\n","episode 107: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00021315, eps=0.19243\n","episode 108: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00041151, eps=0.18949\n","episode 109: cumul_reward=10.0, accuracy:0.66667, cumul_delta=0.00030106, eps=0.1866\n","episode 110: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0002957, eps=0.18374\n","episode 111: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.00025749, eps=0.18094\n","episode 112: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.00022405, eps=0.17817\n","episode 113: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00016266, eps=0.17545\n","episode 114: cumul_reward=13.0, accuracy:0.86667, cumul_delta=8.8513e-05, eps=0.17277\n","episode 115: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.00015646, eps=0.17013\n","episode 116: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.00013101, eps=0.16753\n","episode 117: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0001176, eps=0.16496\n","episode 118: cumul_reward=7.0, accuracy:0.46667, cumul_delta=6.5625e-05, eps=0.16244\n","episode 119: cumul_reward=13.0, accuracy:0.86667, cumul_delta=8.589e-05, eps=0.15996\n","episode 120: cumul_reward=15.0, accuracy:1.0, cumul_delta=8.0049e-05, eps=0.15752\n","episode 121: cumul_reward=9.0, accuracy:0.6, cumul_delta=4.7266e-05, eps=0.15511\n","episode 122: cumul_reward=14.0, accuracy:0.93333, cumul_delta=5.9307e-05, eps=0.15274\n","episode 123: cumul_reward=9.0, accuracy:0.6, cumul_delta=3.8147e-05, eps=0.1504\n","episode 124: cumul_reward=13.0, accuracy:0.86667, cumul_delta=4.2558e-05, eps=0.1481\n","episode 125: cumul_reward=12.0, accuracy:0.8, cumul_delta=3.1233e-05, eps=0.14584\n","episode 126: cumul_reward=13.0, accuracy:0.86667, cumul_delta=2.0862e-05, eps=0.14361\n","episode 127: cumul_reward=12.0, accuracy:0.8, cumul_delta=2.7657e-05, eps=0.14142\n","episode 128: cumul_reward=12.0, accuracy:0.8, cumul_delta=1.3471e-05, eps=0.13926\n","episode 129: cumul_reward=14.0, accuracy:0.93333, cumul_delta=2.4617e-05, eps=0.13713\n","episode 130: cumul_reward=13.0, accuracy:0.86667, cumul_delta=2.0146e-05, eps=0.13503\n","episode 131: cumul_reward=13.0, accuracy:0.86667, cumul_delta=1.6272e-05, eps=0.13297\n","episode 132: cumul_reward=13.0, accuracy:0.86667, cumul_delta=1.6332e-05, eps=0.13094\n","episode 133: cumul_reward=12.0, accuracy:0.8, cumul_delta=1.3471e-05, eps=0.12893\n","episode 134: cumul_reward=11.0, accuracy:0.73333, cumul_delta=1.0788e-05, eps=0.12696\n","episode 135: cumul_reward=14.0, accuracy:0.93333, cumul_delta=1.061e-05, eps=0.12502\n","episode 136: cumul_reward=12.0, accuracy:0.8, cumul_delta=7.8678e-06, eps=0.12311\n","episode 137: cumul_reward=13.0, accuracy:0.86667, cumul_delta=3.5763e-06, eps=0.12123\n","episode 138: cumul_reward=11.0, accuracy:0.73333, cumul_delta=6.4969e-06, eps=0.11938\n","episode 139: cumul_reward=12.0, accuracy:0.8, cumul_delta=5.9605e-06, eps=0.11755\n","episode 140: cumul_reward=12.0, accuracy:0.8, cumul_delta=4.4703e-06, eps=0.11576\n","episode 141: cumul_reward=13.0, accuracy:0.86667, cumul_delta=3.8147e-06, eps=0.11399\n","episode 142: cumul_reward=15.0, accuracy:1.0, cumul_delta=4.3511e-06, eps=0.11225\n","episode 143: cumul_reward=15.0, accuracy:1.0, cumul_delta=3.7551e-06, eps=0.11053\n","episode 144: cumul_reward=12.0, accuracy:0.8, cumul_delta=2.3842e-06, eps=0.10884\n","episode 145: cumul_reward=14.0, accuracy:0.93333, cumul_delta=2.4438e-06, eps=0.10718\n","episode 146: cumul_reward=14.0, accuracy:0.93333, cumul_delta=2.265e-06, eps=0.10554\n","episode 147: cumul_reward=13.0, accuracy:0.86667, cumul_delta=1.2517e-06, eps=0.10393\n","episode 148: cumul_reward=15.0, accuracy:1.0, cumul_delta=1.7881e-06, eps=0.10234\n","episode 149: cumul_reward=13.0, accuracy:0.86667, cumul_delta=1.2517e-06, eps=0.10077\n","episode 150: cumul_reward=15.0, accuracy:1.0, cumul_delta=1.3113e-06, eps=0.099233\n","episode 151: cumul_reward=13.0, accuracy:0.86667, cumul_delta=8.9407e-07, eps=0.097716\n","episode 152: cumul_reward=14.0, accuracy:0.93333, cumul_delta=8.9407e-07, eps=0.096223\n","episode 153: cumul_reward=15.0, accuracy:1.0, cumul_delta=7.1526e-07, eps=0.094752\n","episode 154: cumul_reward=13.0, accuracy:0.86667, cumul_delta=4.7684e-07, eps=0.093304\n","episode 155: cumul_reward=14.0, accuracy:0.93333, cumul_delta=5.3644e-07, eps=0.091878\n","episode 156: cumul_reward=13.0, accuracy:0.86667, cumul_delta=4.1723e-07, eps=0.090474\n","episode 157: cumul_reward=14.0, accuracy:0.93333, cumul_delta=3.5763e-07, eps=0.089091\n","episode 158: cumul_reward=13.0, accuracy:0.86667, cumul_delta=2.3842e-07, eps=0.087729\n","episode 159: cumul_reward=12.0, accuracy:0.8, cumul_delta=1.7881e-07, eps=0.086388\n","episode 160: cumul_reward=15.0, accuracy:1.0, cumul_delta=2.3842e-07, eps=0.085068\n","episode 161: cumul_reward=15.0, accuracy:1.0, cumul_delta=1.7881e-07, eps=0.083768\n","episode 162: cumul_reward=13.0, accuracy:0.86667, cumul_delta=1.1921e-07, eps=0.082487\n","episode 163: cumul_reward=15.0, accuracy:1.0, cumul_delta=5.9605e-08, eps=0.081227\n","episode 164: cumul_reward=13.0, accuracy:0.86667, cumul_delta=5.9605e-08, eps=0.079985\n","episode 165: cumul_reward=15.0, accuracy:1.0, cumul_delta=5.9605e-08, eps=0.078763\n","episode 166: cumul_reward=13.0, accuracy:0.86667, cumul_delta=5.9605e-08, eps=0.077559\n","episode 167: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.076374\n","episode 168: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.075206\n","episode 169: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.074057\n","episode 170: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.072925\n","episode 171: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.07181\n","episode 172: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.070713\n","episode 173: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.069632\n","episode 174: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.068568\n","episode 175: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.06752\n","episode 176: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.066488\n","episode 177: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.065472\n","episode 178: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0, eps=0.064471\n","episode 179: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0, eps=0.063486\n","episode 180: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.062515\n","episode 181: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.06156\n","episode 182: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.060619\n","episode 183: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0, eps=0.059692\n","episode 184: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.05878\n","episode 185: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.057882\n","episode 186: cumul_reward=12.0, accuracy:0.8, cumul_delta=0.0, eps=0.056997\n","episode 187: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.056126\n","episode 188: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.055268\n","episode 189: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.054423\n","episode 190: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.053592\n","episode 191: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.052773\n","episode 192: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.051966\n","episode 193: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.051172\n","episode 194: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.05039\n","episode 195: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.049619\n","episode 196: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.048861\n","episode 197: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.048114\n","episode 198: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.047379\n","episode 199: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.046655\n","episode 200: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.045942\n","episode 201: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.04524\n","episode 202: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.044548\n","episode 203: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.043867\n","episode 204: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.043197\n","episode 205: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.042537\n","episode 206: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.041886\n","episode 207: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.041246\n","episode 208: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.040616\n","episode 209: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.039995\n","episode 210: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.039384\n","episode 211: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.038782\n","episode 212: cumul_reward=13.0, accuracy:0.86667, cumul_delta=0.0, eps=0.038189\n","episode 213: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.037605\n","episode 214: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.037031\n","episode 215: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.036465\n","episode 216: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.035907\n","episode 217: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.035359\n","episode 218: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.034818\n","episode 219: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.034286\n","episode 220: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.033762\n","episode 221: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.033246\n","episode 222: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.032738\n","episode 223: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.032237\n","episode 224: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.031745\n","episode 225: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.03126\n","episode 226: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.030782\n","episode 227: cumul_reward=14.0, accuracy:0.93333, cumul_delta=0.0, eps=0.030311\n","episode 228: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.029848\n","episode 229: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.029392\n","episode 230: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.028943\n","episode 231: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.0285\n","episode 232: cumul_reward=15.0, accuracy:1.0, cumul_delta=0.0, eps=0.028065\n"],"name":"stdout"}]}]}